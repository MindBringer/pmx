# PMX â€“ Modularer KI-Stack mit HTTPS

Ein leichtgewichtiger, modularer KI-Stack mit Docker Compose, Ollama & n8n.

---

## ğŸ“¦ Dienste

```
pmx/
â”œâ”€â”€ ollama (http://localhost:11434)
â”œâ”€â”€ n8n (https://ai.steinicke-gmbh.de)
â”œâ”€â”€ install.sh
â”œâ”€â”€ install_ssl.sh
â””â”€â”€ n8n-workflow/prompting_wf.json
```

---

## ğŸ›  Installation

### 1. Basis-Installation

```bash
git clone https://github.com/MindBringer/pmx.git
cd pmx
cp .env.example .env
chmod +x install.sh
./install.sh
```

### 2. HTTPS aktivieren (Let's Encrypt mit Fallback auf Self-Signed)

```bash
chmod +x install_ssl.sh
sudo ./install_ssl.sh
```
> Voraussetzung: Domain `ai.steinicke-gmbh.de` zeigt per DNS auf die Ã¶ffentliche IP des Servers (A-Record).


---

## ğŸ” Sicherheit & Cookie-Konfiguration

- Nach `install_ssl.sh` ist TLS aktiv.
- `.env` enthÃ¤lt automatisch `N8N_SECURE_COOKIE=true`, sodass Cookies korrekt Ã¼ber HTTPS gesetzt werden.

---

## ğŸ§  Beispielaufruf (curl)

```bash
curl -X POST https://ai.steinicke-gmbh.de/webhook/prompt \
  -H "Content-Type: application/json" \
  -u admin:supersecure \
  -d '{
    "prompt": "Was ist DevOps?",
    "model": "ollama"
  }'
```

---

## ğŸ“˜ Modelle & API

| Modell    | Ziel-API                                    |
|-----------|---------------------------------------------|
| `ollama`  | http://ollama:11434/api/generate            |
| `openai`  | https://api.openai.com/v1/chat/completions  |

### BenÃ¶tigte Parameter:
- `prompt` â€“ Eingabetext
- `model` â€“ `openai` oder `ollama`

---

## ğŸ” Dienste steuern

```bash
docker compose ps          # Status
docker compose restart     # Neustart
./install_ssl.sh           # (Re-)Einrichtung TLS + Cookie
```

---

## ğŸŒ HTTPS-Domain vorbereiten

Bei deinem DNS-Provider:

| Typ | Name                  | Wert                |
|-----|-----------------------|---------------------|
| A   | ai.domain.de | <deine Ã¶ffentliche IP> |

> ğŸ” Port 80 & 443 mÃ¼ssen Ã¶ffentlich erreichbar sein.

---

## ğŸ”„ Erweiterbar fÃ¼r

- Weitere Modelle (Ã¼ber Ollama oder eigene Container)
- Frontends per Webhook/API
- Authentifizierung via OAuth/OpenID
- Logging mit Datenbank oder Dateisystem

---

## âœ… Fertig

Der Stack ist nach Installation Ã¼ber TLS verfÃ¼gbar:

```
https://ai.domain.de
https://ai.local
```

Login: Benutzer/Passwort aus `.env`

curl --no-progress-meter --max-time 60 --retry 2 --retry-delay 3 \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Gib mir ein Beispiel fÃ¼r einen HTTP POST mit curl", "model": "ollama"}' \
  https://ai.local/webhook/llm

  SETUP frontend:
  bash /frontend/install.sh, nginx anpassen sudo nano /etc/nginx/sites-available/ai.local
  neustarten:
  sudo nginx -t && sudo systemctl reload nginx
