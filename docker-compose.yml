services:
  # === 1. OLLAMA – immer aktiv ======================================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    command: serve

  # === 2. vLLM (nur GPU / AVX‑512) ==================================
  vllm:
    profiles: ["vllm"]
    image: ${VLLM_IMAGE:-vllm/vllm-openai:0.9.1}   # GPU‑Image default
    runtime: ${VLLM_RUNTIME:-nvidia}               # wird ignoriert bei CPU‑Image
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
      - VLLM_LOGGING_LEVEL=INFO
      - VLLM_CPU_KVCACHE_SPACE=32
    command: >
      python -m vllm.entrypoints.openai.api_server
        --model mistralai/Mistral-7B-Instruct-v0.2
        --model meta-llama/Meta-Llama-3-8B-Instruct
        --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5

  # === 3. HF TGI (CPU‑Fallback) ======================================
  tgi:
    profiles: ["tgi"]
    image: ghcr.io/huggingface/text-generation-inference:latest
    restart: unless-stopped
    ports:
      - "8000:80"                # angeglichen an vLLM‑Port
    volumes:
      - tgi_data:/data
    environment:
      - MODEL_ID=mistralai/Mistral-7B-Instruct-v0.2
      - SECONDARY_MODEL_ID=meta-llama/Meta-Llama-3-8B-Instruct
      - NUM_SHARD=1
      - MAX_BATCH_PREFILL_TOKENS=4096
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # === 4. gateway_chatgpt – proxy zu OpenAI ==========================
  gateway_chatgpt:
    image: ghcr.io/berriai/litellm:main
    restart: unless-stopped
    ports:
      - "8080:8000"
    environment:
      - LITELLM_API_KEY=${OPENAI_API_KEY}
      - LITELLM_DEFAULT_MODEL=chatgpt
      - LITELLM_PORT=8000
      - LITELLM_CONFIG_PATH=/app/config.yaml
    volumes:
      - ./config.yaml:/app/config.yaml

  # === 5. gateway_local – proxy zu lokalem Backend ===================
  gateway_local:
    image: ghcr.io/berriai/litellm:main
    restart: unless-stopped
    ports:
      - "8090:8000"
    environment:
      - LITELLM_DEFAULT_MODEL=mistral-vllm
      - LITELLM_PORT=8000
      - LITELLM_CONFIG_PATH=/app/config.yaml
    volumes:
      - ./config.yaml:/app/config.yaml
    depends_on:
      - ollama

volumes:
  ollama_data:
  tgi_data:
