# docker compose --profile svc up -d    # auf srv-aisvc (Services-VM)
# docker compose --profile ai  up -d    # auf srv-ai01  (GPU-VM)

services:
  # =========================
  # AI-VM (srv-ai01) Profile
  # =========================
  ollama:
    profiles: ["ai"]
    image: ollama/ollama
    container_name: ollama
    ports: ["11434:11434"]
    volumes:
      - ollama-data:/root/.ollama
      - ollama-models:/root/.ollama
    environment:
      - OLLAMA_MODELS=mistral
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all              # ODER: device_ids: ["0"] (nicht beides)
              capabilities: ["gpu"]
    restart: unless-stopped
    networks: [ai-net]

  audio-api:
    profiles: ["ai"]
    build:
      context: .
      dockerfile: rag-backend/Dockerfile.audio
    container_name: audio-api
    ports: ["6080:6080"]              # /transcribe, /speakers
    deploy: {}   # (kein Swarm nötig)
    gpus: all
    environment:
      - DEVICE=cuda
      - ASR_MODEL=medium
      - ASR_COMPUTE_TYPE=float16
      - QDRANT_URL=${QDRANT_URL_AI:-http://192.168.30.42:6333}
      - QDRANT_API_KEY=${QDRANT_API_KEY:-}
      - SPEAKER_COLLECTION=speakers
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:6080/health || curl -fsS http://localhost:6080/openapi.json >/dev/null"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 20s
    restart: unless-stopped
    networks: [ai-net]

# ==============
  # vLLM – Allrounder
  # ==============
  vllm-allrounder:
    profiles: ["ai", "ai-vllm"]
    image: vllm/vllm-openai:latest
    container_name: vllm-allrounder
    ports: ["8001:8000"]
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}     # export HF_TOKEN=... (oder in .env)
    command:
      - --model=${VLLM_MODEL_ALLROUNDER}
      - --gpu-memory-utilization=${VLLM_GPU_UTIL}
      - --max-model-len=4096
      - --trust-remote-code
      - --download-dir=/models
    volumes:
      - vllm-models:/models
      - vllm-cache:/root/.cache/huggingface
    depends_on:
      audio-api:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/v1/models"]
      interval: 30s
      timeout: 5s
      retries: 10
    restart: unless-stopped
    networks: [ai-net]

  # =================================
  # Services-VM (srv-aisvc) Profile
  # =================================
  n8n:
    profiles: ["svc"]
    image: n8nio/n8n
    container_name: n8n
    ports: ["${N8N_PORT:-5678}:5678"]
    environment:
      - GENERIC_TIMEZONE=Europe/Berlin
      - N8N_HOST=ai.intern
      - N8N_EDITOR_BASE_URL=https://ai.intern/
      - WEBHOOK_URL=https://ai.intern/
      # Optional Basic Auth (falls gesetzt)
      - N8N_BASIC_AUTH_ACTIVE=${N8N_BASIC_AUTH_ACTIVE:-false}
      - N8N_BASIC_AUTH_USER=${N8N_USER:-}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_PASS:-}
      # Summarize und Transcribe
      - N8N_DEFAULT_BINARY_DATA_MODE=memory
      # optional, je nach Größe deiner Audios:
      - N8N_BINARY_DATA_TTL=7200            # 2h
      - N8N_BINARY_DATA_MAX_SIZE=200000000  # 200 MB
      # Optional: direkte Targets zur AI-VM
      - OLLAMA_GPU0_URL=${OLLAMA_GPU0_URL:-http://192.168.30.43:11434}
    extra_hosts:
      - "ollama:192.168.30.43"
    volumes:
      - n8n-data:/home/node/.n8n
    restart: unless-stopped
    networks: [ai-net]

  qdrant:
    profiles: ["svc"]
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports: ["6333:6333"]
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped
    networks: [ai-net]

  rag-backend:
    profiles: ["svc"]
    build:
      context: ./rag-backend
      dockerfile: ./Dockerfile
    pull_policy: build
    container_name: rag-backend
    env_file:
      - ./rag-backend/.env
    environment:
      RAG_BASE_PATH: /rag
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://192.168.30.43:11434}
      QDRANT_URL:      ${QDRANT_URL_SVC:-http://qdrant:6333}
    ports: ["8082:8082"]
    # --- BASE: Named Volumes (robust, skriptunabhängig) ---
    volumes:
      - rag_documents:/app/documents
      - rag_storage:/app/storage
    depends_on:
      qdrant:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8082/health"]
      interval: 20s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks: [ai-net]

volumes:
  ollama-data:
  ollama-models:
  vllm-models:
  vllm-cache:
  n8n-data:
  qdrant_data:
  rag_documents:
  rag_storage:

networks:
  ai-net:
