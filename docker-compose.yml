# docker compose --profile ai up -d     # auf srv-ai01 (GPU-VM)
# docker compose --profile svc up -d    # auf srv-aisvc (Services-VM)

services:
  # =========================
  # AI-VM (srv-ai01) Profile
  # =========================
  ollama:
    profiles: ["ai"]
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      # Hinweis: zwei Mounts auf den gleichen Pfad überlagern sich; belassen wir so, um dein Setup nicht zu brechen.
      - ollama-data:/root/.ollama
      - ollama-models:/root/.ollama
    restart: unless-stopped
    environment:
      - OLLAMA_MODELS=mistral
    networks:
      - ai-net

  # Audio-API extrahiert aus rag-backend (nutzt deine transcribe/speakers-Logik)
  # Erwartet, dass im Dockerfile/Entrypoint der FastAPI-Wrapper gestartet wird (siehe unsere vorherigen Hinweise).
  audio-api:
    profiles: ["ai"]
    build:
      context: ./rag-backend
      dockerfile: ./Dockerfile.audio
    container_name: audio-api
    ports:
      - "6080:6080"
    environment:
      - DEVICE=cuda
      - ASR_MODEL=medium
      - ASR_COMPUTE_TYPE=float16
      - QDRANT_URL=http://192.168.30.42:6333
      - QDRANT_API_KEY=${QDRANT_API_KEY:-}
      - SPEAKER_COLLECTION=speakers
      # → Nvidia CTK: hilft der Runtime beim richtigen Setup
      - NVIDIA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              device_ids: ["0"]
              capabilities: ["gpu"]
    restart: unless-stopped
    networks:
      - ai-net

  # =================================
  # Services-VM (srv-aisvc) Profile
  # =================================
  n8n:
    profiles: ["svc"]
    image: n8nio/n8n
    container_name: n8n
    ports:
      - "${N8N_PORT}:5678"
    restart: unless-stopped
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_USER}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_PASS}
      - GENERIC_TIMEZONE=Europe/Berlin
      - N8N_SECURE_COOKIE=${N8N_SECURE_COOKIE}
      - N8N_LOG_LEVEL=debug
      - EXECUTIONS_DATA_SAVE_ON_PROGRESS=true
      - EXECUTIONS_DATA_SAVE_SUCCESS_EXECUTIONS=all
      - EXECUTIONS_DATA_SAVE_FAILED_EXECUTIONS=all
      - EXECUTIONS_DATA_SAVE_MANUAL_EXECUTIONS=true
      # Optional: Endpunkte zur AI-VM, falls deine Flows direkt auf Ollama zeigen sollen
      - OLLAMA_GPU0_URL=http://192.168.30.43:11434
      # - OLLAMA_GPU1_URL=http://192.168.30.43:11435
    volumes:
      - n8n-data:/home/node/.n8n
    # KEIN depends_on auf ollama, da ollama auf anderer VM läuft
    networks:
      - ai-net

  qdrant:
    profiles: ["svc"]
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped
    networks:
      - ai-net

  rag-backend:
    profiles: ["svc"]
    build:
      context: ./rag-backend
      dockerfile: ./Dockerfile
    pull_policy: build
    container_name: rag-backend
    env_file:
      - ./rag-backend/.env
    environment:
      # Fallbacks (bleiben wie gehabt)
      RAG_BASE_PATH: /rag
      # WICHTIG: Ollama steht jetzt auf der AI-VM
      OLLAMA_BASE_URL: http://192.168.30.43:11434
      QDRANT_URL: http://qdrant:6333
    ports:
      - "8082:8082"
    volumes:
      - ./rag-backend/documents:/app/documents
      - ./rag-backend/storage:/app/storage
    depends_on:
      # nur qdrant lokal; KEIN depends_on auf ollama, da remote
      qdrant:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8082/rag/health"]
      interval: 20s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - ai-net

volumes:
  ollama-data:
  ollama-models:
  n8n-data:
  rag_documents:
  rag_storage:
  qdrant_data:

networks:
  ai-net:
