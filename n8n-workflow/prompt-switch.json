{
  "name": "LLM Router with Respond",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "llm",
        "responseMode": "responseNode"
      },
      "id": "webhook-node",
      "name": "Webhook",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [100, 300],
      "webhookId": "llm-router"
    },
    {
      "parameters": {
        "jsCode": "const { prompt, model, openai_api_key = null } = $input.json;\n\nif (!prompt || !model) {\n  throw new Error('prompt and model are required');\n}\n\nreturn {\n  prompt,\n  model,\n  openai_api_key\n};"
      },
      "id": "input-check-node",
      "name": "Input Check",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [300, 300]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "openai-condition",
              "leftValue": "={{ $json.model }}",
              "rightValue": "openai",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            },
            {
              "id": "ollama-condition", 
              "leftValue": "={{ $json.model }}",
              "rightValue": "ollama",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ]
        },
        "options": {}
      },
      "id": "model-switch-node",
      "name": "Model Switch",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [500, 300]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.openai.com/v1/chat/completions",
        "authentication": "genericCredentialType",
        "genericAuthType": "httpHeaderAuth",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Authorization",
              "value": "Bearer {{ $json.openai_api_key }}"
            },
            {
              "name": "Content-Type", 
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "bodyParameters": {
          "parameters": [
            {
              "name": "model",
              "value": "gpt-3.5-turbo"
            },
            {
              "name": "messages",
              "value": [
                {
                  "role": "user",
                  "content": "={{ $json.prompt }}"
                }
              ]
            }
          ]
        },
        "options": {}
      },
      "id": "openai-node",
      "name": "Call OpenAI",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [700, 200]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://ollama:11434/api/generate",
        "sendBody": true,
        "bodyParameters": {
          "parameters": [
            {
              "name": "model",
              "value": "mistral"
            },
            {
              "name": "prompt",
              "value": "={{ $json.prompt }}"
            },
            {
              "name": "stream",
              "value": false
            }
          ]
        },
        "options": {}
      },
      "id": "ollama-node", 
      "name": "Call Ollama",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [700, 400]
    },
    {
      "parameters": {
        "jsCode": "// Extrahiere die Antwort je nach API\nconst data = $input.json;\n\nlet result;\nif (data.choices && data.choices[0] && data.choices[0].message) {\n  // OpenAI Format\n  result = data.choices[0].message.content;\n} else if (data.response) {\n  // Ollama Format\n  result = data.response;\n} else {\n  // Fallback\n  result = JSON.stringify(data);\n}\n\nreturn {\n  result: result,\n  status: 'success'\n};"
      },
      "id": "format-node",
      "name": "Format Response",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [900, 300]
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ $json }}"
      },
      "id": "respond-node",
      "name": "Respond to Webhook",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [1100, 300]
    }
  ],
  "connections": {
    "Webhook": {
      "main": [
        [
          {
            "node": "Input Check",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Input Check": {
      "main": [
        [
          {
            "node": "Model Switch",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Model Switch": {
      "main": [
        [
          {
            "node": "Call OpenAI",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Call Ollama",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call OpenAI": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Ollama": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Response": {
      "main": [
        [
          {
            "node": "Respond to Webhook",
            "type": "main", 
            "index": 0
          }
        ]
      ]
    }
  },
  "active": true,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "1",
  "meta": {
    "templateCredsSetupCompleted": true
  },
  "id": "llm-router-workflow",
  "tags": []
}