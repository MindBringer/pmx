{
  "name": "RAG Router V3",
  "nodes": [
    {
      "parameters": {},
      "id": "c8dc5252-caca-4ed2-a693-6d56e7aea87a",
      "name": "Execute Workflow Trigger",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1,
      "position": [
        -3136,
        1040
      ]
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "keep-model",
              "name": "model",
              "value": "={{ $json.model }}",
              "type": "string"
            },
            {
              "id": "keep-prompt",
              "name": "prompt",
              "value": "={{ $json.prompt }}",
              "type": "string"
            },
            {
              "id": "keep-system",
              "name": "system",
              "value": "={{ $json.system || '' }}",
              "type": "string"
            },
            {
              "id": "keep-rag",
              "name": "rag",
              "value": "={{ $json.rag }}",
              "type": "string"
            },
            {
              "id": "keep-topk",
              "name": "rag_top_k",
              "value": "={{ Number($json.rag_top_k) || 5 }}",
              "type": "string"
            },
            {
              "id": "keep-thr",
              "name": "rag_score_threshold",
              "value": "={{ Number($json.rag_score_threshold) || 0.45 }}",
              "type": "string"
            },
            {
              "id": "keep-key",
              "name": "rag_api_key",
              "value": "={{ $json.rag_api_key }}",
              "type": "string"
            },
            {
              "id": "keep-tags",
              "name": "rag_tags",
              "value": "={{ Array.isArray($json.rag_tags) ? $json.rag_tags : [] }}",
              "type": "string"
            },
            {
              "id": "keep-hf",
              "name": "hf_model",
              "value": "={{ $json.hf_model }}",
              "type": "string"
            },
            {
              "id": "keep-openai",
              "name": "openai_model",
              "value": "={{ $json.openai_model }}",
              "type": "string"
            },
            {
              "id": "keep-claude",
              "name": "claude_model",
              "value": "={{ $json.claude_model }}",
              "type": "string"
            },
            {
              "id": "keep-ollama",
              "name": "ollama_model",
              "value": "={{ $json.ollama_model || $json.model || 'llama3' }}",
              "type": "string"
            },
            {
              "id": "keep-mistral",
              "name": "mistral_model",
              "value": "={{ $json.mistral_model }}",
              "type": "string"
            },
            {
              "id": "4b68771f-f185-4c92-8c8b-01d51033f615",
              "name": "conversation_id",
              "value": "={{ $json.conversation_id }}",
              "type": "string"
            },
            {
              "id": "8d6c6a97-8eff-4657-9a35-a8b7e6ddec74",
              "name": "job_id",
              "value": "={{ $json.job_id }}",
              "type": "string"
            }
          ]
        },
        "includeOtherFields": true,
        "options": {}
      },
      "id": "de2f321d-cad7-46c5-bb6e-66730c38f97d",
      "name": "Context (original fields)",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -2912,
        1040
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "no-rag",
              "leftValue": "={{ $json.rag }}",
              "rightValue": "true",
              "operator": {
                "type": "string",
                "operation": "notEquals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "41d4686f-391d-4ac1-91c2-761d6631848e",
      "name": "without rag?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -2688,
        1040
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://rag-backend:8082/query",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            },
            {
              "name": "x-api-key",
              "value": "={{ $('Context (original fields)').item.json.rag_api_key }}"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  query: $('Context (original fields)').item.json.prompt,\n  top_k: Number($('Context (original fields)').item.json.rag_top_k) || 3,\n  score_threshold: Number($('Context (original fields)').item.json.rag_score_threshold) || 0.45,\n  tags: Array.isArray($('Context (original fields)').item.json.rag_tags) ? $('Context (original fields)').item.json.rag_tags : []\n}) }}",
        "options": {
          "timeout": 300000
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -2464,
        1136
      ],
      "id": "ecc233ec-fb83-49a2-ae3b-94cb8678b527",
      "name": "RAG Query"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Build Augmented Prompt — per-item safe + Memory injection\nconst j = ($json && typeof $json === 'object') ? $json : {};\n\n// 1) Basis-Prompt (unverfälscht, falls vorhanden)\nconst basePrompt = j._original_prompt ?? j.prompt ?? \"\";\n\n// 2) Memory aufbereiten\nconst mem = j.memory || {};\nconst summary = (mem.summary || \"\").trim();\nconst history = Array.isArray(mem.history) ? mem.history.slice(-8) : []; // letzte 8 Einträge\n\nfunction fmt(m) {\n  const role = (m.role === 'assistant') ? 'Assistant' : 'User';\n  return `${role}: ${String(m.content || '').trim()}`;\n}\nconst histText = history.map(fmt).join('\\n');\n\nconst memParts = [];\nif (summary) memParts.push(`Zusammenfassung:\\n${summary}`);\nif (histText) memParts.push(`Letzte Nachrichten:\\n${histText}`);\n\nconst memoryBlock = memParts.length\n  ? `\\n\\n# Gesprächsspeicher\\n${memParts.join('\\n\\n')}`\n  : \"\";\n\n// 3) RAG-Kontext (falls vorhanden)\nlet docs = [];\nif (Array.isArray(j.docs)) docs = j.docs;\nelse if (Array.isArray(j.documents)) docs = j.documents;\nelse if (Array.isArray(j.matches)) docs = j.matches.map(m => m.document || m);\n\nfunction toEntry(d){\n  const text = d.text || d.content || d.page_content || d.pageContent || d.body || '';\n  const meta = d.metadata || d.meta || {};\n  const src  = meta.source || meta.file || meta.path || meta.url || meta.name || '';\n  return { text: String(text), source: String(src) };\n}\nconst entries = docs.map(toEntry).filter(e => e.text);\nconst contextText = entries.map((e, i) => `#${i+1} ${e.text}`).join('\\n\\n');\n\nconst sources = entries\n  .map((e, i) => ({ id: i+1, source: e.source }))\n  .filter(s => s.source);\n\nconst used_tags = Array.isArray(j.used_tags) ? j.used_tags\n                  : (Array.isArray(j.rag_tags) ? j.rag_tags : []);\n\nconst useRag = !!j.rag;\nconst contextBlock = (useRag && contextText)\n  ? `\\n\\n# Kontext (RAG)\\n${contextText}`\n  : \"\";\n\n// 4) Tools einbinden (falls vorhanden)\nconst obs = Array.isArray(j.tool_observations) ? j.tool_observations : [];\nfunction fmtObs(o, idx){\n  if (o.tool === 'web.search' && Array.isArray(o.items)) {\n    const lines = o.items.slice(0,5).map((it,i)=>`- ${it.title || '(ohne Titel)'} — ${it.url || ''}\\n  ${it.snippet ? String(it.snippet).slice(0,240) : ''}`);\n    return `## Tool #${idx+1}: Websuche\\n${lines.join('\\n')}`;\n  }\n  if (o.tool === 'db.query') {\n    const rc = Number(o.rowCount || (o.rows?.length || 0));\n    const sample = Array.isArray(o.rows) ? o.rows.slice(0, 1)[0] : null;\n    return `## Tool #${idx+1}: DB-Query (${rc} Zeilen)\\n` + (sample ? \"Beispiel:\\n\" + JSON.stringify(sample, null, 2) : \"\");\n  }\n  if (o.tool === 'program.run') {\n    const out = String(o.stdout || '').slice(0, 1500);\n    const note = o.truncated ? '\\n[... gekürzt ...]' : '';\n    return `## Tool #${idx+1}: Programm-Ausgabe (Exit ${o.exitCode})\\n${out}${note}`;\n  }\n  return `## Tool #${idx+1}: ${o.tool}\\n${JSON.stringify(o).slice(0, 1000)}`;\n}\nconst toolsBlock = obs.length ? `\\n\\n# Werkzeuge\\n${obs.map(fmtObs).join('\\n\\n')}` : \"\";\n\n// Finaler Prompt inkl. Tools\nconst augmentedPrompt = [basePrompt, memoryBlock, contextBlock, toolsBlock].filter(Boolean).join(\"\");\n\nreturn { json: {\n  ...j,\n  prompt: augmentedPrompt,\n  sources,\n  used_tags\n}};\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -2240,
        1040
      ],
      "id": "0c192374-a654-4b20-afe7-7619fbfc005c",
      "name": "Build Augmented Prompt"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-groq",
              "leftValue": "={{ $json.model }}",
              "rightValue": "groq",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "18ac83c6-1672-4579-8554-1fb9db3cab3c",
      "name": "Is groq?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -1792,
        1040
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-openrouter",
              "leftValue": "={{ $json.model }}",
              "rightValue": "openrouter",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "2cae24cd-3172-45ff-aba9-82cf85d04ad7",
      "name": "Is openrouter?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -1568,
        1232
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-mistralapi",
              "leftValue": "={{ $json.model }}",
              "rightValue": "mistralapi",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "7fa2ff0e-438b-4e53-9eca-0a0367818014",
      "name": "Is mistralAPI?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -1344,
        1376
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-openai",
              "leftValue": "={{ $json.model }}",
              "rightValue": "openai",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "09d260c9-f23a-4656-9247-c84769bbb713",
      "name": "Is OpenAI?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -1120,
        1472
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-anthropic",
              "leftValue": "={{ $json.model }}",
              "rightValue": "anthropic",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "489ccb3c-4b2b-44a2-aba3-a34c2d9767ab",
      "name": "Is Anthropic?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -896,
        1568
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-hf",
              "leftValue": "={{ $json.model }}",
              "rightValue": "huggingface",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "deb78e55-8056-4329-b7b3-83fdafe4b7c9",
      "name": "Is HuggingFace?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -672,
        1664
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-mistral",
              "leftValue": "={{ $json.model }}",
              "rightValue": "mistral",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "13f65a75-0bb9-42a0-92f6-ea583a747341",
      "name": "Is Mistral?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -224,
        1856
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.openai.com/v1/chat/completions",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "openAiApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  model: $json.openai_model,\n  messages: [\n    ...(($('Context (original fields)').first().json.system || '').trim() ? [{ role: 'system', content: $('Context (original fields)').first().json.system }] : []),\n    { role: 'user', content: $json.prompt }\n  ],\n  max_tokens: 1000,\n  temperature: 0.7\n}) }}",
        "options": {}
      },
      "id": "eaba1e6e-4468-44e6-a9fa-5b4237503bb2",
      "name": "Call OpenAI",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [
        64,
        992
      ],
      "credentials": {
        "openAiApi": {
          "id": "URBYmeZhphqt6tWt",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.anthropic.com/v1/messages",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "anthropicApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  model: $json.claude_model,\n  max_tokens: 1000,\n  messages: [{ role: 'user', content: $json.prompt }],\n  ...((($('Context (original fields)').first().json.system || '').trim()) ? { system: $('Context (original fields)').first().json.system } : {})\n}) }}",
        "options": {}
      },
      "id": "d0a5f484-22c0-40c6-ba0f-a3372e38a613",
      "name": "Call Claude",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [
        64,
        1184
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api-inference.huggingface.co/models/{{ $json.hf_model }}",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "huggingFaceApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  inputs: (($('Context (original fields)').first().json.system || '').trim()) ? ($('Context (original fields)').first().json.system + '\\n\\n' + $json.prompt) : $json.prompt,\n  parameters: { max_length: 500, temperature: 0.7, do_sample: true, return_full_text: false }\n}) }}",
        "options": {}
      },
      "id": "216e780b-8085-4f3f-9a2e-ed57a85a05d8",
      "name": "Call Hugging Face",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [
        64,
        1376
      ],
      "credentials": {
        "huggingFaceApi": {
          "id": "93Jn3lteBGqzmg5r",
          "name": "HuggingFaceApi account"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.mistral.ai/v1/chat/completions",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "mistralCloudApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  model: $json.mistral_model,\n  messages: [\n    { role: 'system', content: ($('Context (original fields)').first().json.system || 'Du bist ein hilfsbereiter Assistent.') },\n    { role: 'user', content: $json.prompt }\n  ],\n  max_tokens: 1000,\n  temperature: 0.3\n}) }}",
        "options": {}
      },
      "id": "12970783-acbe-4b93-8e07-8d7fa8eaec5c",
      "name": "Call mistralAPI",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [
        64,
        800
      ],
      "credentials": {
        "mistralCloudApi": {
          "id": "YZutKG1C8HaZG8pe",
          "name": "Mistral Cloud account"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://ollama:11434/api/generate",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  model: ($json.ollama_model) ?? ($('Context (original fields)').item.json.ollama_model ?? $('Context (original fields)').item.json.model ?? 'llama3'),\n  prompt: ($json.prompt) ?? ($('Context (original fields)').item.json.prompt ?? ''),\n  stream: false,\n  options: {\n    temperature: (typeof $json.temperature === 'number') ? $json.temperature : 0.3,\n    top_p:       (typeof $json.top_p === 'number')       ? $json.top_p       : 0.9,\n    num_predict: (typeof $json.num_predict === 'number') ? $json.num_predict : 600\n  }\n}) }}",
        "options": {
          "timeout": 300000
        }
      },
      "id": "02bb6014-9816-4102-a8c9-bad8905eb4de",
      "name": "Call Ollama",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [
        64,
        1760
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://ollama:11434/api/generate",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  model: 'llama3',\n  prompt: (($('Context (original fields)').first().json.system || '').trim()) ? ($('Context (original fields)').first().json.system + '\\n\\n' + $json.prompt) : $json.prompt,\n  stream: false,\n  options: { temperature: 0.7, num_predict: 500 }\n}) }}",
        "options": {}
      },
      "id": "da871d31-01cb-434c-93a7-178d6f929c34",
      "name": "Call Llama (Default)",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [
        64,
        1952
      ]
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "c6cdfd6f-7e91-493c-b4ab-a565d824584c",
              "name": "sources",
              "value": "",
              "type": "string"
            },
            {
              "id": "f3453492-81cc-4ecf-bc2e-66ca20087ff3",
              "name": "used_tags",
              "value": "",
              "type": "string"
            }
          ]
        },
        "includeOtherFields": true,
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -2464,
        944
      ],
      "id": "062fd1a0-dee4-415a-a870-7fa3033cd724",
      "name": "No RAG Context"
    },
    {
      "parameters": {
        "model": "openai/gpt-oss-120b",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGroq",
      "typeVersion": 1,
      "position": [
        144,
        224
      ],
      "id": "9ed2d84a-779e-484d-bf3f-34d241bef7a2",
      "name": "GPT-OSS-120b",
      "credentials": {
        "groqApi": {
          "id": "DX8FKLrqc231mbg3",
          "name": "Groq account"
        }
      }
    },
    {
      "parameters": {
        "model": "gemma2-9b-it",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGroq",
      "typeVersion": 1,
      "position": [
        144,
        624
      ],
      "id": "b7ef4a63-49cd-4846-ae13-4ce413b06c35",
      "name": "gemma-it",
      "credentials": {
        "groqApi": {
          "id": "DX8FKLrqc231mbg3",
          "name": "Groq account"
        }
      }
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Response Formatter — per-item safe (runOnceForEachItem)\nfunction robustExtractThinksAndStrip(s){\n  const src = String(s || '');\n  const thinks = [];\n  let clean = src.replace(/<think>([\\s\\S]*?)<\\/think>\\s*/gi, (_, t)=>{ const seg=String(t||'').trim(); if(seg)thinks.push(seg); return ''; });\n  clean = clean.replace(/```(?:think|thinking|reasoning)[\\t ]*\\n([\\s\\S]*?)```/gi, (_, t)=>{ const seg=String(t||'').trim(); if(seg)thinks.push(seg); return ''; });\n  clean = clean.replace(/(^|\\n)#{2,3}\\s*think\\b[\\s\\S]*?(?=(\\n#{2,3}\\s)|$)/gi, (m)=>{ const seg=m.replace(/^.*?\\n/,'').trim(); if(seg)thinks.push(seg); return ''; });\n  clean = clean.replace(/(^|\\n)think:\\s*([\\s\\S]*?)(?=\\n\\s*\\n|\\n#{1,6}\\s|$)/gi, (_, __, t)=>{ const seg=String(t||'').trim(); if(seg)thinks.push(seg); return ''; });\n  return { clean: clean.trim(), thinks };\n}\nfunction bulletsFromThinks(thinks){\n  const raw = thinks.join('\\n');\n  const lines = raw.split(/\\r?\\n+/).map(s=>s.trim()).filter(Boolean);\n  const sent  = raw.split(/(?<=[.!?])\\s+/).map(s=>s.trim()).filter(Boolean);\n  const pool  = (lines.length >= 3 ? lines : sent);\n  return pool.slice(0, 6);\n}\n\n// ------ Kontext holen ------\nlet requested = {};\ntry {\n  const ctx = $items('Context (original fields)', 0) || [];\n  requested = (Array.isArray(ctx) && ctx[0] && ctx[0].json) ? ctx[0].json : {};\n} catch { requested = {}; }\n\n// Provider-Rohdaten sind im aktuellen Item:\nlet data = $json;\n\n// ------ Stream-Parser (NDJSON / SSE) ------\nfunction parseNdjsonString(s){\n  try { return String(s).trim().split('\\n').filter(Boolean).map(l=>JSON.parse(l)); }\n  catch { return null; }\n}\nfunction parseSseString(s){\n  const out=[];\n  for (const line of String(s).split('\\n')) {\n    const t=line.trim();\n    if (!t || !t.toLowerCase().startsWith('data:')) continue;\n    const p=t.slice(5).trim();\n    if (!p || p==='[DONE]') continue;\n    try { out.push(JSON.parse(p)); } catch {}\n  }\n  return out.length ? out : null;\n}\nfunction extractTextFromChunk(o){\n  if (typeof o?.response === 'string') return o.response;\n  const c = o?.choices && o.choices[0];\n  if (c) {\n    if (c.delta && typeof c.delta.content === 'string') return c.delta.content;\n    if (c.message && typeof c.message.content === 'string') return c.message.content;\n    if (typeof c.text === 'string') return c.text;\n    if (Array.isArray(c.content)) return c.content.map(x => typeof x.text==='string' ? x.text : '').join('');\n  }\n  if (typeof o?.content === 'string') return o.content;\n  if (typeof o?.output_text === 'string') return o.output_text;\n  if (typeof o?.text === 'string') return o.text;\n  return '';\n}\n\n// ------ Normalisierung & Extraktion ------\nlet result = '';\nlet provider = 'Unknown';\nlet actualModel = requested.model || '';\nlet usage = null;\nlet raw = data;\n\n// Wenn der Provider ein String ist, in {data} packen\nif (typeof data === 'string') { raw={data}; data={data}; }\n\n// Streaming-Formate erkennen\nif (data && typeof data.data === 'string') {\n  let chunks = parseNdjsonString(data.data);\n  if (chunks && chunks.length) {\n    provider = 'Ollama';\n    actualModel = (chunks.find(o=>o.done===true)?.model) || chunks[0]?.model || requested.ollama_model || actualModel;\n    result = chunks.map(extractTextFromChunk).filter(Boolean).join('').trim();\n    raw = { ndjson: chunks };\n  } else {\n    chunks = parseSseString(data.data);\n    if (chunks && chunks.length) {\n      provider = (chunks.find(o=>o?.provider) ? 'OpenRouter' : 'Groq/OpenAI-like');\n      actualModel = (chunks.find(o=>o?.model)?.model) || requested.model || requested.openai_model || requested.ollama_model || actualModel;\n      result = chunks.map(extractTextFromChunk).filter(Boolean).join('').trim();\n      raw = { sse: chunks };\n    }\n  }\n}\n\n// Non-Streaming-Formate (inkl. vLLM / OpenAI-like)\nif (!result) {\n  if (data && data.choices?.[0]?.message) {\n    // OpenAI-kompatibel -> bei uns i.d.R. vLLM\n    result = data.choices[0].message.content;\n    provider = (requested.llm_target?.includes('allrounder') || requested.llm_target?.includes('base') || requested.model === 'vllm') ? 'vLLM' : 'OpenAI';\n    actualModel = data.model || requested.openai_model || requested.vLLM_model_allround || requested.vLLM_model_base || actualModel;\n    usage = data.usage || null;\n  } else if (data && Array.isArray(data.content) && data.content[0]) {\n    result = data.content[0].text || data.content[0].content;\n    provider = 'Claude/Anthropic';\n    actualModel = data.model || requested.claude_model || actualModel;\n  } else if (Array.isArray(data) && data[0]?.generated_text) {\n    result = data[0].generated_text;\n    provider = 'Hugging Face';\n    actualModel = requested.hf_model || actualModel;\n  } else if (data && data.generated_text) {\n    result = data.generated_text;\n    provider = 'Hugging Face';\n    actualModel = requested.hf_model || actualModel;\n  } else if (data && typeof data.response === 'string') {\n    result = data.response;\n    provider = 'Ollama';\n    actualModel = data.model || requested.ollama_model || actualModel;\n  } else if (data && data.text) {\n    result = data.text;\n    provider = 'OpenRouter/Groq-like';\n    actualModel = requested.model || requested.openai_model || requested.ollama_model || actualModel;\n  } else if (data && data.error) {\n    result = `Error: ${data.error.message || JSON.stringify(data.error)}`;\n    provider = 'Error';\n    actualModel = 'N/A';\n  } else {\n    result = `Unexpected response format: ${JSON.stringify(data)}`;\n    provider = 'Unknown';\n    actualModel = 'N/A';\n  }\n}\n\n// ------ Quellen, Tags & Label übernehmen ------\nconst sources = Array.isArray(requested.sources) ? requested.sources\n              : (Array.isArray($json.sources) ? $json.sources : []);\nconst used_tags = Array.isArray(requested.used_tags) ? requested.used_tags\n                : (Array.isArray(requested.rag_tags) ? requested.rag_tags\n                   : (Array.isArray($json.used_tags) ? $json.used_tags : []));\nconst inputLabel = requested.label ?? $json.label;\n\n// --- Thinks raus, Answer setzen ---\nconst { clean, thinks } = robustExtractThinksAndStrip(result);\nconst artifacts = {};\nif (thinks.length) {\n  artifacts.rationale_summary = { persona: [], writer: bulletsFromThinks(thinks) };\n}\n\n// Format Response — array-safe\nconst item = $json || {};\n\nconst out = {\n  ok: (item.status || 'success') !== 'error',\n  status: item.status || 'success',\n  provider: item.provider || provider || 'unknown',\n  model_used: item.model_used || actualModel || item.model || 'unknown',\n\n  // Wichtig: den normalisierten Text nehmen!\n  answer: (clean || result || item.answer || item.text || ''),\n\n  // die zuvor ermittelten Quellen/Tags nutzen\n  sources,\n  used_tags,\n\n  // Artefakte/Usage zusammenführen\n  artifacts: { ...(item.artifacts || {}), ...(artifacts || {}) },\n  usage: item.usage || usage || null,\n\n  conversation_id: item.conversation_id || '',\n  label: item.label || undefined,\n  timestamp: item.timestamp || new Date().toISOString(),\n\n  // rohdaten mitgeben, hilft beim Debuggen\n  raw,\n};\n// --- Helpers (falls noch nicht definiert) ---\nfunction get(o, p, d=''){ try{ return p.split('.').reduce((a,k)=>a?.[k], o) ?? d; }catch{ return d; } }\nfunction pickStr(...vals){ for (const v of vals) if (typeof v==='string' && v.trim()) return v.trim(); return ''; }\n\nfunction normalize(j){\n  const flat =\n    (typeof j.result === 'object') ? j.result :\n    (typeof j.data   === 'object') ? j.data   : j;\n\n  const openaiLike    = pickStr(get(flat,'choices.0.message.content'), get(flat,'choices.0.text'));\n  const anthropicLike = Array.isArray(flat?.content)\n      ? pickStr(flat.content.map(b => b.text).filter(Boolean).join('\\n'))\n      : pickStr(flat.completion, flat.output_text);\n  const hfLike        = Array.isArray(flat) ? pickStr(flat[0]?.generated_text) : pickStr(flat.generated_text);\n  const simple        = pickStr(flat.answer, flat.text, j.answer, j.text);\n\n  const clean = pickStr(openaiLike, anthropicLike, hfLike, simple);\n  const sources =\n    Array.isArray(j.sources) ? j.sources :\n    Array.isArray(flat?.sources) ? flat.sources : [];\n  const artifacts = { ...(j.artifacts||{}), ...(flat?.artifacts||{}) };\n\n  return {\n    ok: (j.status || 'success') !== 'error',\n    status: j.status || 'success',\n    provider: j.provider || 'unknown',\n    model_used: j.model_used || j.actualModel || j.model || 'unknown',\n    answer: clean,\n    sources,\n    used_tags: Array.isArray(j.used_tags) ? j.used_tags : [],\n    artifacts,\n    usage: j.usage || null,\n    conversation_id: j.conversation_id || '',\n    timestamp: j.timestamp || new Date().toISOString(),\n    raw: flat,\n  };\n}\n\n// --- Modus erkennen & korrekt zurückgeben ---\nconst inEachItemMode = (typeof items === 'undefined');\n\nif (inEachItemMode) {\n  // \"Run once for each item\" → EIN Objekt zurückgeben\n  return { json: normalize($json) };\n} else {\n  // \"Run once for all items\" → ARRAY von Items zurückgeben\n  return items.map(({ json }) => ({ json: normalize(json) }));\n}\n\n"
      },
      "id": "fda2b9dd-f148-409d-9eaa-2f529d051abf",
      "name": "Format Response",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        352,
        1184
      ]
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "={{ (($('Context (original fields)').first().json.system || '').trim()) ? $('Context (original fields)').first().json.system + \"\\n\\n\" + $json.prompt : $json.prompt }}",
        "needsFallback": true,
        "batching": {}
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.7,
      "position": [
        0,
        400
      ],
      "id": "9f40d726-d8bc-4f0b-b2af-55776e45298a",
      "name": "openrouter"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "={{ (($('Context (original fields)').first().json.system || '').trim()) ? $('Context (original fields)').first().json.system + \"\\n\\n\" + $json.prompt : $json.prompt }}",
        "needsFallback": true,
        "batching": {}
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.7,
      "position": [
        0,
        0
      ],
      "id": "0be17daa-d936-4da8-a686-8133dc2e0d29",
      "name": "Groq"
    },
    {
      "parameters": {
        "model": "qwen/qwen3-32b",
        "options": {
          "maxTokensToSample": 1500,
          "temperature": 0.3
        }
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGroq",
      "typeVersion": 1,
      "position": [
        16,
        224
      ],
      "id": "5e32520d-880c-43fa-9d96-7d5a61709eb9",
      "name": "Qwen3",
      "credentials": {
        "groqApi": {
          "id": "DX8FKLrqc231mbg3",
          "name": "Groq account"
        }
      }
    },
    {
      "parameters": {
        "model": "llama-3.3-70b-versatile",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGroq",
      "typeVersion": 1,
      "position": [
        16,
        624
      ],
      "id": "ee6b4d2d-5cf5-48ed-95cd-6bc880d20c79",
      "name": "Llama3",
      "credentials": {
        "groqApi": {
          "id": "DX8FKLrqc231mbg3",
          "name": "Groq account"
        }
      }
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// ---- Konfiguration ----\nconst TPM_LIMIT = 10000;          // dein Tier-Limit (on_demand-Beispiel)\nconst SAFETY    = 0.90;           // Sicherheitsmarge 90%\nconst MAX_OUT   = 1500;           // geplante Antwortobergrenze (auch an Groq senden)\n\n// Grobe Token-Schätzung: ~4 Zeichen ≈ 1 Token für lateinische Sprachen\nconst tok = s => Math.ceil(String(s||'').length / 4);\n\n// Prompt holen\nlet prompt = $json.prompt || \"\";\n\n// Budget berechnen\nconst budgetIn = Math.floor(TPM_LIMIT * SAFETY) - MAX_OUT;\nlet inTok = tok(prompt);\n\n// Falls zu groß: zuerst den RAG-Kontext abspecken\nif (inTok > budgetIn) {\n  // Wir erkennen deinen Kontextblock \"# Kontext (RAG)\"\n  const SPLIT = \"\\n\\n# Kontext (RAG)\\n\";\n  const parts = prompt.split(SPLIT);\n  if (parts.length === 2) {\n    let head = parts[0];\n    let ctx  = parts[1];\n\n    // \"Einträge\" (#1, #2, ...) herauslösen und iterativ kürzen\n    const entries = ctx.split(/\\n\\n#\\d+\\s/).map((t,i)=> i===0 ? t : '#'+(i)+' '+t).filter(Boolean);\n    // Mindestbudget für head\n    const headTok = tok(head);\n    let remainTok = Math.max(budgetIn - headTok, 0);\n\n    let kept = [];\n    for (const e of entries) {\n      if (tok(kept.join(\"\\n\\n\") + \"\\n\\n\" + e) <= remainTok) kept.push(e);\n      else break;\n    }\n\n    // Falls immer noch zu groß: hart abschneiden\n    let newCtx = kept.join(\"\\n\\n\");\n    if (tok(head + SPLIT + newCtx) > budgetIn) {\n      const maxChars = Math.max((budgetIn - headTok) * 4, 0);\n      newCtx = newCtx.slice(0, maxChars) + \"\\n\\n[Kontext gekürzt]\";\n    }\n\n    prompt = head + SPLIT + newCtx;\n  } else {\n    // Kein erkennbarer Kontextblock -> tail-kürzen\n    const maxChars = Math.max(budgetIn * 4, 0);\n    prompt = \"[Prompt gekürzt]\\n\" + prompt.slice(-maxChars);\n  }\n  inTok = tok(prompt);\n}\n\n// Meta mitgeben\nreturn {\n  json: {\n    ...$json,\n    prompt,\n    token_budget: {\n      tpm_limit: TPM_LIMIT,\n      safety: SAFETY,\n      max_completion_tokens: MAX_OUT,\n      input_estimate: inTok,\n      input_budget: budgetIn,\n      capped: inTok > budgetIn ? true : false\n    }\n  }\n};\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -2016,
        1040
      ],
      "id": "213facd7-fc01-40f0-9bcc-26939d96b502",
      "name": "Token Budget Gate"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "const reqCtx = ($items('Context (original fields)', 0)?.[0]?.json) || {};\nconst res = $json || {};\n\n// Text extrahieren\nlet out = '';\nif (Array.isArray(res.choices) && res.choices[0]) {\n  const c = res.choices[0];\n  out = (c.message?.content) || c.text || '';\n}\n\n// Denk-Abschnitte entfernen\nout = String(out||'')\n  .replace(/<think>[\\s\\S]*?<\\/think>\\s*/gi,'')\n  .replace(/```(?:think|thinking|reasoning)[\\t ]*\\n[\\s\\S]*?```/gi,'')\n  .trim();\n\n// *** EINZELNES OBJEKT zurückgeben ***\nreturn {\n  json: {\n    provider: 'vllm',\n    model_used: res.model || reqCtx.vLLM_model_allround || reqCtx.model || 'vllm',\n    answer: out,\n    usage: res.usage || undefined,\n    raw: res\n  }\n};"
      },
      "id": "6547267d-83ab-4ab1-8161-df3c9fbfb31d",
      "name": "Format vLLM Result",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        64,
        1568
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-hf",
              "leftValue": "={{ $json.model }}",
              "rightValue": "vllm",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "625f8cfc-5ca6-4e19-939e-51f49fa3de1d",
      "name": "Is vLLM?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -448,
        1760
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://192.168.30.43:8001/v1/chat/completions",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  model: $json.vLLM_model || $env.VLLM_MODEL_ALLROUNDER || 'Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4',\n  messages: [\n    ...((($('Context (original fields)').first().json.system || '').trim()) ? [{ role:'system', content: $('Context (original fields)').first().json.system }] : []),\n    { role:'user', content: $json.prompt }\n  ],\n  temperature: Number.isFinite($json.temperature) ? $json.temperature : 0.3,\n  top_p: Number.isFinite($json.top_p) ? $json.top_p : 0.9,\n  max_tokens: Number.isFinite($json.max_tokens) ? $json.max_tokens : 800,\n  stream: false\n}) }}",
        "options": {
          "timeout": 300000
        }
      },
      "id": "54712d43-ef4f-43e7-affc-c8fabbb02cf6",
      "name": "Call vLLM",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -224,
        1568
      ]
    }
  ],
  "pinData": {},
  "connections": {
    "Execute Workflow Trigger": {
      "main": [
        [
          {
            "node": "Context (original fields)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Context (original fields)": {
      "main": [
        [
          {
            "node": "without rag?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "without rag?": {
      "main": [
        [
          {
            "node": "No RAG Context",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "RAG Query",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "RAG Query": {
      "main": [
        [
          {
            "node": "Build Augmented Prompt",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Augmented Prompt": {
      "main": [
        [
          {
            "node": "Token Budget Gate",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is groq?": {
      "main": [
        [
          {
            "node": "Groq",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Is openrouter?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is openrouter?": {
      "main": [
        [
          {
            "node": "openrouter",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Is mistralAPI?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is mistralAPI?": {
      "main": [
        [
          {
            "node": "Call mistralAPI",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Is OpenAI?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is OpenAI?": {
      "main": [
        [
          {
            "node": "Call OpenAI",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Is Anthropic?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is Anthropic?": {
      "main": [
        [
          {
            "node": "Call Claude",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Is HuggingFace?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is HuggingFace?": {
      "main": [
        [
          {
            "node": "Call Hugging Face",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Is vLLM?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is Mistral?": {
      "main": [
        [
          {
            "node": "Call Ollama",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Call Llama (Default)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call OpenAI": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Claude": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Hugging Face": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call mistralAPI": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Ollama": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Llama (Default)": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "No RAG Context": {
      "main": [
        [
          {
            "node": "Build Augmented Prompt",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "GPT-OSS-120b": {
      "ai_languageModel": [
        [
          {
            "node": "Groq",
            "type": "ai_languageModel",
            "index": 1
          }
        ]
      ]
    },
    "gemma-it": {
      "ai_languageModel": [
        [
          {
            "node": "openrouter",
            "type": "ai_languageModel",
            "index": 1
          }
        ]
      ]
    },
    "openrouter": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Groq": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Qwen3": {
      "ai_languageModel": [
        [
          {
            "node": "Groq",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Llama3": {
      "ai_languageModel": [
        [
          {
            "node": "openrouter",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Token Budget Gate": {
      "main": [
        [
          {
            "node": "Is groq?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format vLLM Result": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is vLLM?": {
      "main": [
        [
          {
            "node": "Call vLLM",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Is Mistral?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call vLLM": {
      "main": [
        [
          {
            "node": "Format vLLM Result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "2a2eff67-0618-49d2-8e6b-3dc3bf7574cb",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "561919f222f248fefb13e03c84adfc7356dad806feb0489acfa0f3688a2a1852"
  },
  "id": "b40VJ560cdjZeCOR",
  "tags": [
    {
      "createdAt": "2025-09-11T13:23:00.506Z",
      "updatedAt": "2025-09-11T13:23:00.506Z",
      "id": "4ZXhh7HxNdYuxid7",
      "name": "Subflow"
    }
  ]
}