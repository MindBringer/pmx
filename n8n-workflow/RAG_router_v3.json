{
  "name": "RAG_router_v3",
  "nodes": [
    {
      "parameters": {},
      "id": "4e89858a-f6c8-4c8e-a5b1-a3d187a1cfcb",
      "name": "Execute Workflow Trigger",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1,
      "position": [
        -2688,
        944
      ]
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "keep-model",
              "name": "model",
              "value": "={{ $json.model }}",
              "type": "string"
            },
            {
              "id": "keep-prompt",
              "name": "prompt",
              "value": "={{ $json.prompt }}",
              "type": "string"
            },
            {
              "id": "keep-system",
              "name": "system",
              "value": "={{ $json.system || '' }}",
              "type": "string"
            },
            {
              "id": "keep-rag",
              "name": "rag",
              "value": "={{ $json.rag }}",
              "type": "string"
            },
            {
              "id": "keep-topk",
              "name": "rag_top_k",
              "value": "={{ Number($json.rag_top_k) || 5 }}",
              "type": "string"
            },
            {
              "id": "keep-thr",
              "name": "rag_score_threshold",
              "value": "={{ Number($json.rag_score_threshold) || 0.45 }}",
              "type": "string"
            },
            {
              "id": "keep-key",
              "name": "rag_api_key",
              "value": "={{ $json.rag_api_key }}",
              "type": "string"
            },
            {
              "id": "keep-tags",
              "name": "rag_tags",
              "value": "={{ Array.isArray($json.rag_tags) ? $json.rag_tags : [] }}",
              "type": "string"
            },
            {
              "id": "keep-hf",
              "name": "hf_model",
              "value": "={{ $json.hf_model }}",
              "type": "string"
            },
            {
              "id": "keep-openai",
              "name": "openai_model",
              "value": "={{ $json.openai_model }}",
              "type": "string"
            },
            {
              "id": "keep-claude",
              "name": "claude_model",
              "value": "={{ $json.claude_model }}",
              "type": "string"
            },
            {
              "id": "keep-ollama",
              "name": "ollama_model",
              "value": "={{ $json.ollama_model || $json.model || 'llama3' }}",
              "type": "string"
            },
            {
              "id": "keep-mistral",
              "name": "mistral_model",
              "value": "={{ $json.mistral_model }}",
              "type": "string"
            },
            {
              "id": "4b68771f-f185-4c92-8c8b-01d51033f615",
              "name": "conversation_id",
              "value": "={{ $json.conversation_id }}",
              "type": "string"
            },
            {
              "id": "8d6c6a97-8eff-4657-9a35-a8b7e6ddec74",
              "name": "job_id",
              "value": "={{ $json.job_id }}",
              "type": "string"
            }
          ]
        },
        "includeOtherFields": true,
        "options": {}
      },
      "id": "b400fcdf-de80-4d7f-a422-3236edd4b65a",
      "name": "Context (original fields)",
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -2464,
        944
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "no-rag",
              "leftValue": "={{ $json.rag }}",
              "rightValue": "true",
              "operator": {
                "type": "string",
                "operation": "notEquals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "36975b74-0b6f-45d4-ac2c-8a7901fae188",
      "name": "without rag?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -2240,
        944
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://rag-backend:8082/query",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            },
            {
              "name": "x-api-key",
              "value": "={{ $('Context (original fields)').item.json.rag_api_key }}"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  query: $('Context (original fields)').item.json.prompt,\n  top_k: Number($('Context (original fields)').item.json.rag_top_k) || 3,\n  score_threshold: Number($('Context (original fields)').item.json.rag_score_threshold) || 0.45,\n  tags: Array.isArray($('Context (original fields)').item.json.rag_tags) ? $('Context (original fields)').item.json.rag_tags : []\n}) }}",
        "options": {
          "timeout": 300000
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        -2016,
        1040
      ],
      "id": "97c7ea64-38d5-48c5-a616-d61823a4a43d",
      "name": "RAG Query"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Build Augmented Prompt — per-item safe + Memory injection\nconst j = ($json && typeof $json === 'object') ? $json : {};\n\n// 1) Basis-Prompt (unverfälscht, falls vorhanden)\nconst basePrompt = j._original_prompt ?? j.prompt ?? \"\";\n\n// 2) Memory aufbereiten\nconst mem = j.memory || {};\nconst summary = (mem.summary || \"\").trim();\nconst history = Array.isArray(mem.history) ? mem.history.slice(-8) : []; // letzte 8 Einträge\n\nfunction fmt(m) {\n  const role = (m.role === 'assistant') ? 'Assistant' : 'User';\n  return `${role}: ${String(m.content || '').trim()}`;\n}\nconst histText = history.map(fmt).join('\\n');\n\nconst memParts = [];\nif (summary) memParts.push(`Zusammenfassung:\\n${summary}`);\nif (histText) memParts.push(`Letzte Nachrichten:\\n${histText}`);\n\nconst memoryBlock = memParts.length\n  ? `\\n\\n# Gesprächsspeicher\\n${memParts.join('\\n\\n')}`\n  : \"\";\n\n// 3) RAG-Kontext (falls vorhanden)\nlet docs = [];\nif (Array.isArray(j.docs)) docs = j.docs;\nelse if (Array.isArray(j.documents)) docs = j.documents;\nelse if (Array.isArray(j.matches)) docs = j.matches.map(m => m.document || m);\n\nfunction toEntry(d){\n  const text = d.text || d.content || d.page_content || d.pageContent || d.body || '';\n  const meta = d.metadata || d.meta || {};\n  const src  = meta.source || meta.file || meta.path || meta.url || meta.name || '';\n  return { text: String(text), source: String(src) };\n}\nconst entries = docs.map(toEntry).filter(e => e.text);\nconst contextText = entries.map((e, i) => `#${i+1} ${e.text}`).join('\\n\\n');\n\nconst sources = entries\n  .map((e, i) => ({ id: i+1, source: e.source }))\n  .filter(s => s.source);\n\nconst used_tags = Array.isArray(j.used_tags) ? j.used_tags\n                  : (Array.isArray(j.rag_tags) ? j.rag_tags : []);\n\nconst useRag = !!j.rag;\nconst contextBlock = (useRag && contextText)\n  ? `\\n\\n# Kontext (RAG)\\n${contextText}`\n  : \"\";\n\n// 4) Tools einbinden (falls vorhanden)\nconst obs = Array.isArray(j.tool_observations) ? j.tool_observations : [];\nfunction fmtObs(o, idx){\n  if (o.tool === 'web.search' && Array.isArray(o.items)) {\n    const lines = o.items.slice(0,5).map((it,i)=>`- ${it.title || '(ohne Titel)'} — ${it.url || ''}\\n  ${it.snippet ? String(it.snippet).slice(0,240) : ''}`);\n    return `## Tool #${idx+1}: Websuche\\n${lines.join('\\n')}`;\n  }\n  if (o.tool === 'db.query') {\n    const rc = Number(o.rowCount || (o.rows?.length || 0));\n    const sample = Array.isArray(o.rows) ? o.rows.slice(0, 1)[0] : null;\n    return `## Tool #${idx+1}: DB-Query (${rc} Zeilen)\\n` + (sample ? \"Beispiel:\\n\" + JSON.stringify(sample, null, 2) : \"\");\n  }\n  if (o.tool === 'program.run') {\n    const out = String(o.stdout || '').slice(0, 1500);\n    const note = o.truncated ? '\\n[... gekürzt ...]' : '';\n    return `## Tool #${idx+1}: Programm-Ausgabe (Exit ${o.exitCode})\\n${out}${note}`;\n  }\n  return `## Tool #${idx+1}: ${o.tool}\\n${JSON.stringify(o).slice(0, 1000)}`;\n}\nconst toolsBlock = obs.length ? `\\n\\n# Werkzeuge\\n${obs.map(fmtObs).join('\\n\\n')}` : \"\";\n\n// Finaler Prompt inkl. Tools\nconst augmentedPrompt = [basePrompt, memoryBlock, contextBlock, toolsBlock].filter(Boolean).join(\"\");\n\nreturn { json: {\n  ...j,\n  prompt: augmentedPrompt,\n  sources,\n  used_tags\n}};\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1792,
        944
      ],
      "id": "d7f37b11-f439-483e-8f6d-bb78e000671f",
      "name": "Build Augmented Prompt"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-groq",
              "leftValue": "={{ $json.model }}",
              "rightValue": "groq",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "8f08a79c-d948-4334-9c3f-8ffe9e0c01d9",
      "name": "Is groq?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -1360,
        944
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-openrouter",
              "leftValue": "={{ $json.model }}",
              "rightValue": "openrouter",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "3ce39500-555f-449e-bb4c-9a72c6faa9d2",
      "name": "Is openrouter?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -928,
        1136
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-mistralapi",
              "leftValue": "={{ $json.model }}",
              "rightValue": "mistralapi",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "af90758a-b5b3-45be-bb1b-ef5cd7e4bd05",
      "name": "Is mistralAPI?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -704,
        1280
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-openai",
              "leftValue": "={{ $json.model }}",
              "rightValue": "openai",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "81f0d0f0-8a68-4fcf-92f3-ba8fad049f37",
      "name": "Is OpenAI?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -480,
        1376
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-anthropic",
              "leftValue": "={{ $json.model }}",
              "rightValue": "anthropic",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "64a8112a-581a-4275-b2d4-e7963e4fd145",
      "name": "Is Anthropic?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -256,
        1472
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-hf",
              "leftValue": "={{ $json.model }}",
              "rightValue": "huggingface",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "ce078acc-6da4-4b85-bc38-f7811690933c",
      "name": "Is HuggingFace?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        -32,
        1568
      ]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 1
          },
          "conditions": [
            {
              "id": "is-mistral",
              "leftValue": "={{ $json.model }}",
              "rightValue": "mistral",
              "operator": {
                "type": "string",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "id": "c67f4047-fffc-493a-be70-329b6629379e",
      "name": "Is Mistral?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        192,
        1664
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.openai.com/v1/chat/completions",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "openAiApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  model: $json.openai_model,\n  messages: [\n    ...(($('Context (original fields)').first().json.system || '').trim() ? [{ role: 'system', content: $('Context (original fields)').first().json.system }] : []),\n    { role: 'user', content: $json.prompt }\n  ],\n  max_tokens: 1000,\n  temperature: 0.7\n}) }}",
        "options": {}
      },
      "id": "77517c03-72e9-4246-8c8e-822c5009ed39",
      "name": "Call OpenAI",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [
        480,
        992
      ],
      "credentials": {
        "openAiApi": {
          "id": "xpFTPLasPN9qcrAU",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.anthropic.com/v1/messages",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "anthropicApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  model: $json.claude_model,\n  max_tokens: 1000,\n  messages: [{ role: 'user', content: $json.prompt }],\n  ...((($('Context (original fields)').first().json.system || '').trim()) ? { system: $('Context (original fields)').first().json.system } : {})\n}) }}",
        "options": {}
      },
      "id": "c9526557-e087-4695-b2fd-1bc6d98964cd",
      "name": "Call Claude",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [
        480,
        1184
      ],
      "credentials": {
        "anthropicApi": {
          "id": "NwPymGLQdk29P5fi",
          "name": "Anthropic account"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api-inference.huggingface.co/models/{{ $json.hf_model }}",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "huggingFaceApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  inputs: (($('Context (original fields)').first().json.system || '').trim()) ? ($('Context (original fields)').first().json.system + '\\n\\n' + $json.prompt) : $json.prompt,\n  parameters: { max_length: 500, temperature: 0.7, do_sample: true, return_full_text: false }\n}) }}",
        "options": {}
      },
      "id": "54071bd9-3d9c-4a95-8d56-512ffdfb54b9",
      "name": "Call Hugging Face",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [
        480,
        1376
      ],
      "credentials": {
        "huggingFaceApi": {
          "id": "QO87qgUFRrbiQqXv",
          "name": "HuggingFaceApi account"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "https://api.mistral.ai/v1/chat/completions",
        "authentication": "predefinedCredentialType",
        "nodeCredentialType": "mistralCloudApi",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  model: $json.mistral_model,\n  messages: [\n    { role: 'system', content: ($('Context (original fields)').first().json.system || 'Du bist ein hilfsbereiter Assistent.') },\n    { role: 'user', content: $json.prompt }\n  ],\n  max_tokens: 1000,\n  temperature: 0.3\n}) }}",
        "options": {}
      },
      "id": "6cf0a06e-f580-4773-b4fc-3bde1dd6b2e0",
      "name": "Call mistralAPI",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [
        480,
        800
      ],
      "credentials": {
        "mistralCloudApi": {
          "id": "cp078sOI6OYLKqiI",
          "name": "Mistral Cloud API-key"
        }
      }
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://ollama:11434/api/generate",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  model: ($json.ollama_model) ?? ($('Context (original fields)').item.json.ollama_model ?? $('Context (original fields)').item.json.model ?? 'llama3'),\n  prompt: ($json.prompt) ?? ($('Context (original fields)').item.json.prompt ?? ''),\n  stream: false,\n  options: {\n    temperature: (typeof $json.temperature === 'number') ? $json.temperature : 0.3,\n    top_p:       (typeof $json.top_p === 'number')       ? $json.top_p       : 0.9,\n    num_predict: (typeof $json.num_predict === 'number') ? $json.num_predict : 600\n  }\n}) }}",
        "options": {
          "timeout": 300000
        }
      },
      "id": "54edbcb8-7cde-43d3-a49d-e91eaa2c01cc",
      "name": "Call Ollama",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [
        480,
        1568
      ]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://ollama:11434/api/generate",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ ({\n  model: 'llama3',\n  prompt: (($('Context (original fields)').first().json.system || '').trim()) ? ($('Context (original fields)').first().json.system + '\\n\\n' + $json.prompt) : $json.prompt,\n  stream: false,\n  options: { temperature: 0.7, num_predict: 500 }\n}) }}",
        "options": {}
      },
      "id": "72be1f85-6fc9-44df-a080-fee4e8a7b039",
      "name": "Call Llama (Default)",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [
        480,
        1760
      ]
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "c6cdfd6f-7e91-493c-b4ab-a565d824584c",
              "name": "sources",
              "value": "",
              "type": "string"
            },
            {
              "id": "f3453492-81cc-4ecf-bc2e-66ca20087ff3",
              "name": "used_tags",
              "value": "",
              "type": "string"
            }
          ]
        },
        "includeOtherFields": true,
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        -2016,
        848
      ],
      "id": "17675d88-b0fc-4e8d-9aa3-1d1efb3828d0",
      "name": "No RAG Context"
    },
    {
      "parameters": {
        "model": "openai/gpt-oss-120b",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGroq",
      "typeVersion": 1,
      "position": [
        560,
        224
      ],
      "id": "d83c61d6-6ee3-4338-936f-f9fa6ca4415f",
      "name": "GPT-OSS-120b",
      "credentials": {
        "groqApi": {
          "id": "mhumLW1ZXnpB1MfM",
          "name": "Groq account"
        }
      }
    },
    {
      "parameters": {
        "model": "gemma2-9b-it",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGroq",
      "typeVersion": 1,
      "position": [
        560,
        624
      ],
      "id": "1eb79001-e890-4a6a-bfd5-c16a648d6375",
      "name": "gemma-it",
      "credentials": {
        "groqApi": {
          "id": "mhumLW1ZXnpB1MfM",
          "name": "Groq account"
        }
      }
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Response Formatter — per-item safe (no .first(), no .item())\n// Liest den Original-Kontext über $items('Context (original fields)') ein,\n// damit prompt/system/conversation_id zuverlässig vorhanden sind – auch wenn\n// der Provider-Call $json überschreibt.\n\nfunction robustExtractThinksAndStrip(s){\n  const src = String(s || '');\n  const thinks = [];\n  let clean = src.replace(/<think>([\\s\\S]*?)<\\/think>\\s*/gi, (_, t)=>{ const seg=String(t||'').trim(); if(seg)thinks.push(seg); return ''; });\n  clean = clean.replace(/```(?:think|thinking|reasoning)[\\t ]*\\n([\\s\\S]*?)```/gi, (_, t)=>{ const seg=String(t||'').trim(); if(seg)thinks.push(seg); return ''; });\n  clean = clean.replace(/(^|\\n)#{2,3}\\s*think\\b[\\s\\S]*?(?=(\\n#{2,3}\\s)|$)/gi, (m)=>{ const seg=m.replace(/^.*?\\n/,'').trim(); if(seg)thinks.push(seg); return ''; });\n  clean = clean.replace(/(^|\\n)think:\\s*([\\s\\S]*?)(?=\\n\\s*\\n|\\n#{1,6}\\s|$)/gi, (_, __, t)=>{ const seg=String(t||'').trim(); if(seg)thinks.push(seg); return ''; });\n  return { clean: clean.trim(), thinks };\n}\n\nfunction bulletsFromThinks(thinks){\n  const raw = thinks.join('\\n');\n  const lines = raw.split(/\\r?\\n+/).map(s=>s.trim()).filter(Boolean);\n  const sent  = raw.split(/(?<=[.!?])\\s+/).map(s=>s.trim()).filter(Boolean);\n  const pool  = (lines.length >= 3 ? lines : sent);\n  return pool.slice(0, 6);\n}\n\n//// ------ Kontext holen ------\nlet requested = {};\ntry {\n  const ctx = $items('Context (original fields)', 0) || [];\n  requested = (Array.isArray(ctx) && ctx[0] && ctx[0].json) ? ctx[0].json : {};\n} catch { requested = {}; }\n\n// Provider-Rohdaten sind im aktuellen Item:\nlet data = $json;\n\n//// ------ Stream-Parser (NDJSON / SSE) ------\nfunction parseNdjsonString(s){\n  try {\n    const lines = String(s).trim().split('\\n').filter(Boolean);\n    return lines.map(l=>JSON.parse(l));\n  } catch { return null; }\n}\nfunction parseSseString(s){\n  const out=[];\n  for (const line of String(s).split('\\n')) {\n    const t=line.trim();\n    if (!t || !t.toLowerCase().startsWith('data:')) continue;\n    const p=t.slice(5).trim();\n    if (!p || p==='[DONE]') continue;\n    try { out.push(JSON.parse(p)); } catch {}\n  }\n  return out.length ? out : null;\n}\nfunction extractTextFromChunk(o){\n  if (typeof o?.response === 'string') return o.response;\n  const c = o?.choices && o.choices[0];\n  if (c) {\n    if (c.delta && typeof c.delta.content === 'string') return c.delta.content;\n    if (c.message && typeof c.message.content === 'string') return c.message.content;\n    if (typeof c.text === 'string') return c.text;\n    if (Array.isArray(c.content)) return c.content.map(x => typeof x.text==='string' ? x.text : '').join('');\n  }\n  if (typeof o?.content === 'string') return o.content;\n  if (typeof o?.output_text === 'string') return o.output_text;\n  if (typeof o?.text === 'string') return o.text;\n  return '';\n}\n\n//// ------ Normalisierung & Extraktion ------\nlet result = '';\nlet provider = 'Unknown';\nlet actualModel = requested.model || '';\nlet usage = null;\nlet raw = data;\n\n// Wenn der Provider ein String ist, in {data} packen\nif (typeof data === 'string') { raw={data}; data={data}; }\n\n// Streaming-Formate erkennen\nif (data && typeof data.data === 'string') {\n  let chunks = parseNdjsonString(data.data);\n  if (chunks && chunks.length) {\n    provider = 'Ollama';\n    actualModel = (chunks.find(o=>o.done===true)?.model) || chunks[0]?.model || requested.ollama_model || actualModel;\n    result = chunks.map(extractTextFromChunk).filter(Boolean).join('').trim();\n    raw = { ndjson: chunks };\n  } else {\n    chunks = parseSseString(data.data);\n    if (chunks && chunks.length) {\n      provider = (chunks.find(o=>o?.provider) ? 'OpenRouter' : 'Groq/OpenAI-like');\n      actualModel = (chunks.find(o=>o?.model)?.model) || requested.model || requested.openai_model || requested.ollama_model || actualModel;\n      result = chunks.map(extractTextFromChunk).filter(Boolean).join('').trim();\n      raw = { sse: chunks };\n    }\n  }\n}\n\n// Non-Streaming-Formate\nif (!result) {\n  if (data && data.choices?.[0]?.message) {\n    // OpenAI-like\n    result = data.choices[0].message.content;\n    provider = 'OpenAI';\n    actualModel = data.model || requested.openai_model || actualModel;\n    usage = data.usage || null;\n  } else if (data && Array.isArray(data.content) && data.content[0]) {\n    // Anthropic-like\n    result = data.content[0].text || data.content[0].content;\n    provider = 'Claude/Anthropic';\n    actualModel = data.model || requested.claude_model || actualModel;\n  } else if (Array.isArray(data) && data[0]?.generated_text) {\n    // HF pipeline (array)\n    result = data[0].generated_text;\n    provider = 'Hugging Face';\n    actualModel = requested.hf_model || actualModel;\n  } else if (data && data.generated_text) {\n    // HF pipeline (object)\n    result = data.generated_text;\n    provider = 'Hugging Face';\n    actualModel = requested.hf_model || actualModel;\n  } else if (data && typeof data.response === 'string') {\n    // Ollama\n    result = data.response;\n    provider = 'Ollama';\n    actualModel = data.model || requested.ollama_model || actualModel;\n  } else if (data && data.text) {\n    // OpenRouter/Groq-like plain\n    result = data.text;\n    provider = 'OpenRouter/Groq-like';\n    actualModel = requested.model || requested.openai_model || requested.ollama_model || actualModel;\n  } else if (data && data.error) {\n    result = `Error: ${data.error.message || JSON.stringify(data.error)}`;\n    provider = 'Error';\n    actualModel = 'N/A';\n  } else {\n    result = `Unexpected response format: ${JSON.stringify(data)}`;\n    provider = 'Unknown';\n    actualModel = 'N/A';\n  }\n}\n\n//// ------ Quellen, Tags & Label übernehmen (einmalig!) ------\nconst sources = Array.isArray(requested.sources)\n  ? requested.sources\n  : (Array.isArray($json.sources) ? $json.sources : []);\nconst used_tags = Array.isArray(requested.used_tags)\n  ? requested.used_tags\n  : (Array.isArray(requested.rag_tags) ? requested.rag_tags\n     : (Array.isArray($json.used_tags) ? $json.used_tags : []));\nconst inputLabel = requested.label ?? $json.label;\n\n// --- Thinks raus, Answer setzen ---\nconst { clean, thinks } = robustExtractThinksAndStrip(result);\n\n// Writer-Rationale (System-Zweig hat keine Personas): packe Thinks als Bullets\nconst artifacts = {};\nif (thinks.length) {\n  artifacts.rationale_summary = {\n    persona: [],\n    writer: bulletsFromThinks(thinks)\n  };\n}\n\n// Rückgabe IMMER mit { answer, sources, artifacts } (+result für Backcompat)\nreturn {\n  json: {\n    answer: clean,\n    result: clean,               // Backward-compat für Stellen, die noch 'result' lesen\n    status: (provider === 'Error') ? 'error' : 'success',\n    provider,\n    model_requested: requested.model,\n    model_used: actualModel,\n    timestamp: new Date().toISOString(),\n    usage,\n    raw_response: raw,\n    sources,\n    used_tags,\n    original_prompt: requested._original_prompt ?? requested.prompt ?? $json.original_prompt ?? $json.prompt,\n    conversation_id: requested.conversation_id ?? $json.conversation_id,\n    label: inputLabel,\n    artifacts\n  }\n};\n"
      },
      "id": "4c354ee7-9cde-4727-b8e6-d5038672d9c4",
      "name": "Format Response",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        768,
        1088
      ]
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "={{ (($('Context (original fields)').first().json.system || '').trim()) ? $('Context (original fields)').first().json.system + \"\\n\\n\" + $json.prompt : $json.prompt }}",
        "needsFallback": true,
        "batching": {}
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.7,
      "position": [
        416,
        400
      ],
      "id": "99f7ec4d-07d7-42f9-8404-8dfd3a049dc7",
      "name": "openrouter"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "={{ (($('Context (original fields)').first().json.system || '').trim()) ? $('Context (original fields)').first().json.system + \"\\n\\n\" + $json.prompt : $json.prompt }}",
        "needsFallback": true,
        "batching": {}
      },
      "type": "@n8n/n8n-nodes-langchain.chainLlm",
      "typeVersion": 1.7,
      "position": [
        416,
        0
      ],
      "id": "ab76abd5-0396-419d-8917-73fd4b6eb6bd",
      "name": "Groq"
    },
    {
      "parameters": {
        "model": "qwen/qwen3-32b",
        "options": {
          "maxTokensToSample": 1500,
          "temperature": 0.3
        }
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGroq",
      "typeVersion": 1,
      "position": [
        432,
        224
      ],
      "id": "d2502226-41dc-4da0-b3bf-458e63aa72d9",
      "name": "Qwen3",
      "credentials": {
        "groqApi": {
          "id": "mhumLW1ZXnpB1MfM",
          "name": "Groq account"
        }
      }
    },
    {
      "parameters": {
        "model": "llama-3.3-70b-versatile",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatGroq",
      "typeVersion": 1,
      "position": [
        432,
        624
      ],
      "id": "05d1a3f1-34fa-4ecc-a619-ccda6146691e",
      "name": "Llama3",
      "credentials": {
        "groqApi": {
          "id": "mhumLW1ZXnpB1MfM",
          "name": "Groq account"
        }
      }
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// ---- Konfiguration ----\nconst TPM_LIMIT = 10000;          // dein Tier-Limit (on_demand-Beispiel)\nconst SAFETY    = 0.90;           // Sicherheitsmarge 90%\nconst MAX_OUT   = 1500;           // geplante Antwortobergrenze (auch an Groq senden)\n\n// Grobe Token-Schätzung: ~4 Zeichen ≈ 1 Token für lateinische Sprachen\nconst tok = s => Math.ceil(String(s||'').length / 4);\n\n// Prompt holen\nlet prompt = $json.prompt || \"\";\n\n// Budget berechnen\nconst budgetIn = Math.floor(TPM_LIMIT * SAFETY) - MAX_OUT;\nlet inTok = tok(prompt);\n\n// Falls zu groß: zuerst den RAG-Kontext abspecken\nif (inTok > budgetIn) {\n  // Wir erkennen deinen Kontextblock \"# Kontext (RAG)\"\n  const SPLIT = \"\\n\\n# Kontext (RAG)\\n\";\n  const parts = prompt.split(SPLIT);\n  if (parts.length === 2) {\n    let head = parts[0];\n    let ctx  = parts[1];\n\n    // \"Einträge\" (#1, #2, ...) herauslösen und iterativ kürzen\n    const entries = ctx.split(/\\n\\n#\\d+\\s/).map((t,i)=> i===0 ? t : '#'+(i)+' '+t).filter(Boolean);\n    // Mindestbudget für head\n    const headTok = tok(head);\n    let remainTok = Math.max(budgetIn - headTok, 0);\n\n    let kept = [];\n    for (const e of entries) {\n      if (tok(kept.join(\"\\n\\n\") + \"\\n\\n\" + e) <= remainTok) kept.push(e);\n      else break;\n    }\n\n    // Falls immer noch zu groß: hart abschneiden\n    let newCtx = kept.join(\"\\n\\n\");\n    if (tok(head + SPLIT + newCtx) > budgetIn) {\n      const maxChars = Math.max((budgetIn - headTok) * 4, 0);\n      newCtx = newCtx.slice(0, maxChars) + \"\\n\\n[Kontext gekürzt]\";\n    }\n\n    prompt = head + SPLIT + newCtx;\n  } else {\n    // Kein erkennbarer Kontextblock -> tail-kürzen\n    const maxChars = Math.max(budgetIn * 4, 0);\n    prompt = \"[Prompt gekürzt]\\n\" + prompt.slice(-maxChars);\n  }\n  inTok = tok(prompt);\n}\n\n// Meta mitgeben\nreturn {\n  json: {\n    ...$json,\n    prompt,\n    token_budget: {\n      tpm_limit: TPM_LIMIT,\n      safety: SAFETY,\n      max_completion_tokens: MAX_OUT,\n      input_estimate: inTok,\n      input_budget: budgetIn,\n      capped: inTok > budgetIn ? true : false\n    }\n  }\n};\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1584,
        944
      ],
      "id": "0a9896b4-d945-47b3-94dd-e5e33da199f5",
      "name": "Token Budget Gate"
    }
  ],
  "pinData": {},
  "connections": {
    "Execute Workflow Trigger": {
      "main": [
        [
          {
            "node": "Context (original fields)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Context (original fields)": {
      "main": [
        [
          {
            "node": "without rag?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "without rag?": {
      "main": [
        [
          {
            "node": "No RAG Context",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "RAG Query",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "RAG Query": {
      "main": [
        [
          {
            "node": "Build Augmented Prompt",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Augmented Prompt": {
      "main": [
        [
          {
            "node": "Token Budget Gate",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is groq?": {
      "main": [
        [
          {
            "node": "Groq",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Is openrouter?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is openrouter?": {
      "main": [
        [
          {
            "node": "openrouter",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Is mistralAPI?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is mistralAPI?": {
      "main": [
        [
          {
            "node": "Call mistralAPI",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Is OpenAI?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is OpenAI?": {
      "main": [
        [
          {
            "node": "Call OpenAI",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Is Anthropic?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is Anthropic?": {
      "main": [
        [
          {
            "node": "Call Claude",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Is HuggingFace?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is HuggingFace?": {
      "main": [
        [
          {
            "node": "Call Hugging Face",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Is Mistral?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is Mistral?": {
      "main": [
        [
          {
            "node": "Call Ollama",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Call Llama (Default)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call OpenAI": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Claude": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Hugging Face": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call mistralAPI": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Ollama": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Call Llama (Default)": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "No RAG Context": {
      "main": [
        [
          {
            "node": "Build Augmented Prompt",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "GPT-OSS-120b": {
      "ai_languageModel": [
        [
          {
            "node": "Groq",
            "type": "ai_languageModel",
            "index": 1
          }
        ]
      ]
    },
    "gemma-it": {
      "ai_languageModel": [
        [
          {
            "node": "openrouter",
            "type": "ai_languageModel",
            "index": 1
          }
        ]
      ]
    },
    "openrouter": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Groq": {
      "main": [
        [
          {
            "node": "Format Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Qwen3": {
      "ai_languageModel": [
        [
          {
            "node": "Groq",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Llama3": {
      "ai_languageModel": [
        [
          {
            "node": "openrouter",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Token Budget Gate": {
      "main": [
        [
          {
            "node": "Is groq?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "65121e45-d031-4b85-9df4-d42bd75c2870",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "5e9f1d1029083353f9423560cb0b3070390ad03855815e064cf1d2e50cd094b4"
  },
  "id": "2cmATLhSNBMN3a2H",
  "tags": [
    {
      "createdAt": "2025-08-15T08:00:40.082Z",
      "updatedAt": "2025-08-15T08:00:40.082Z",
      "id": "MIGc52SFD5ylOxsf",
      "name": "Subflow"
    }
  ]
}